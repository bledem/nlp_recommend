{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_state = 42\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_dir = f'{current_dir}/dataset'\n",
    "dl_folder = f'{current_dir}/dataset/dl'\n",
    "destination_folder = f'{current_dir}/results'\n",
    "filenames = [\n",
    "    'kant.txt', \n",
    "    'aristotle.txt', \n",
    "    'plato.txt', \n",
    "    'hume.txt',\n",
    "    'nietzsche.txt'\n",
    "    ]\n",
    "\n",
    "[os.path.join(data_dir, file) for file in filenames]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>author</th>\n",
       "      <th>tok_lem_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nay suppose we coued draw an inference, it wou...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hume</td>\n",
       "      <td>[nay, suppose, coued, draw, inference, would, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For to know this is true wisdom and virtue, an...</td>\n",
       "      <td>2</td>\n",
       "      <td>Plato</td>\n",
       "      <td>[know, true, wisdom, virtue, ignorance, manife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is the testimony of pleasure of which all...</td>\n",
       "      <td>4</td>\n",
       "      <td>Nietzsche</td>\n",
       "      <td>[testimony, pleasure, religion, proud, althoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And all that is now to be at an end?</td>\n",
       "      <td>4</td>\n",
       "      <td>Nietzsche</td>\n",
       "      <td>[end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xi., Bohns Translation.... In engaging in war ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Kant</td>\n",
       "      <td>[bohns, translation, engaging, war, ought, mak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label     author  \\\n",
       "0  Nay suppose we coued draw an inference, it wou...      1       Hume   \n",
       "1  For to know this is true wisdom and virtue, an...      2      Plato   \n",
       "2  This is the testimony of pleasure of which all...      4  Nietzsche   \n",
       "3               And all that is now to be at an end?      4  Nietzsche   \n",
       "4  xi., Bohns Translation.... In engaging in war ...      0       Kant   \n",
       "\n",
       "                                    tok_lem_sentence  \n",
       "0  [nay, suppose, coued, draw, inference, would, ...  \n",
       "1  [know, true, wisdom, virtue, ignorance, manife...  \n",
       "2  [testimony, pleasure, religion, proud, althoug...  \n",
       "3                                              [end]  \n",
       "4  [bohns, translation, engaging, war, ought, mak...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_data import LoadData\n",
    "\n",
    "corpus = LoadData(n_max=1000)\n",
    "corpus.load()\n",
    "philo_df = corpus.corpus\n",
    "philo_df = philo_df[['sentence', 'label', 'author', 'tok_lem_sentence']]\n",
    "\n",
    "philo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bettyld/anaconda3/envs/py37/lib/python3.7/site-packages/pandas/core/frame.py:1490: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 'Hume', 2: 'Plato', 4: 'Nietzsche', 0: 'Kant', 3: 'Aristotle'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_aut = {}\n",
    "aut_label = philo_df[['author', 'label']].drop_duplicates().reset_index(drop=True).to_dict('record')\n",
    "for k in aut_label:\n",
    "    enc_aut[k['label']] = k['author']\n",
    "enc_aut    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence            0\n",
      "label               0\n",
      "author              0\n",
      "tok_lem_sentence    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bettyld/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEtCAYAAADz1SBvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaE0lEQVR4nO3df7RdZX3n8feHgEDBHzBcMJLURBq1wR/R3jL+mo6KUxgUA444ocqKFY1O46Cj0ylYO+p0snSWVcdxDdpY0IxaaaxSI1oEo0j9UcINhkCAlFSQRDLkiiLRjrEJn/lj72sON+fee+7PfXj257XWXWfv5+x97jdnnXzuPs/e+3lkm4iIKMthTRcQEREzL+EeEVGghHtERIES7hERBUq4R0QUKOEeEVGgw5suAOCEE07wokWLmi4jIuIRZfPmzT+yPdDtub4I90WLFjE0NNR0GRERjyiSfjDWc+mWiYgoUMI9IqJACfeIiAIl3CMiCpRwj4goUMI9IqJACfeIiAIl3CMiCtQXNzHFzFp08ZebLgGAu9/30qZLiGitHLlHRBQo4R4RUaCEe0REgRLuEREFSrhHRBSo53CXNE/S9yRdVa8fL+laSXfWj8d1bHuJpB2Stks6YzYKj4iIsU3myP0twO0d6xcDG20vATbW60haCqwATgXOBC6VNG9myo2IiF70FO6SFgAvBf6io3k5sK5eXgec09F+he19tu8CdgCnzUi1ERHRk16P3P8n8F+AhzraTrK9G6B+PLFuPxnY2bHdrrrtYSStkjQkaWh4eHiydUdExDgmDHdJLwP22N7c42uqS5sPabDX2h60PTgw0HUKwIiImKJehh94PvBySWcBRwGPkfRp4D5J823vljQf2FNvvwtY2LH/AuDemSw6IiLGN+GRu+1LbC+wvYjqROnXbb8G2ACsrDdbCXyxXt4ArJB0pKTFwBJg04xXHhERY5rOwGHvA9ZLuhC4BzgPwPY2SeuB24D9wGrbB6ZdaURE9GxS4W77OuC6evl+4PQxtlsDrJlmbRERMUW5QzUiokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQLxNkHyVpk6SbJW2T9J66/d2SfihpS/1zVsc+l0jaIWm7pDNm8x8QERGH6mUmpn3Ai23/TNIRwLck/W393Ids/1nnxpKWUs21eirwBOBrkp6cqfYiIuZOLxNk2/bP6tUj6h+Ps8ty4Arb+2zfBewATpt2pRER0bOe+twlzZO0BdgDXGv7hvqpN0vaKulyScfVbScDOzt231W3jX7NVZKGJA0NDw9P/V8QERGH6CncbR+wvQxYAJwm6WnAR4FTgGXAbuAD9ebq9hJdXnOt7UHbgwMDA1MoPSIixjKpq2VsPwBcB5xp+7469B8CPs7BrpddwMKO3RYA906/1IiI6FUvV8sMSHpcvXw08BLgDknzOzY7F7i1Xt4ArJB0pKTFwBJg04xWHRER4+rlapn5wDpJ86j+GKy3fZWkT0laRtXlcjfwRgDb2yStB24D9gOrc6VMRMTcmjDcbW8FntWl/YJx9lkDrJleaRERMVW5QzUiokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIK1MtMTEdJ2iTpZknbJL2nbj9e0rWS7qwfj+vY5xJJOyRtl3TGbP4DIiLiUL0cue8DXmz7mVSTYZ8p6TnAxcBG20uAjfU6kpYCK4BTgTOBS+tZnCIiYo5MGO6u/KxePaL+MbAcWFe3rwPOqZeXA1fY3mf7LmAHByfPjoiIOdBTn7ukeZK2AHuAa23fAJxkezdA/XhivfnJwM6O3XfVbRERMUd6CnfbB2wvAxYAp0l62jibq9tLHLKRtErSkKSh4eHhnoqNiIjeTOpqGdsPANdR9aXfJ2k+QP24p95sF7CwY7cFwL1dXmut7UHbgwMDA5OvPCIixtTL1TIDkh5XLx8NvAS4A9gArKw3Wwl8sV7eAKyQdKSkxcASYNMM1x0REeM4vIdt5gPr6iteDgPW275K0neB9ZIuBO4BzgOwvU3SeuA2YD+w2vaB2Sk/IiK6mTDcbW8FntWl/X7g9DH2WQOsmXZ1ERExJblDNSKiQAn3iIgCJdwjIgqUcI+IKFDCPSKiQAn3iIgCJdwjIgqUcI+IKFDCPSKiQAn3iIgCJdwjIgqUcI+IKFDCPSKiQAn3iIgCJdwjIgqUcI+IKFAv0+wtlPQNSbdL2ibpLXX7uyX9UNKW+uesjn0ukbRD0nZJZ8zmPyAiIg7VyzR7+4G3275J0qOBzZKurZ/7kO0/69xY0lJgBXAq8ATga5KenKn2IiLmzoRH7rZ3276pXt4L3A6cPM4uy4ErbO+zfRewAzhtJoqNiIjeTKrPXdIiqvlUb6ib3ixpq6TLJR1Xt50M7OzYbRdd/hhIWiVpSNLQ8PDw5CuPiIgx9Rzuko4FPg+81faDwEeBU4BlwG7gAyObdtndhzTYa20P2h4cGBiYbN0RETGOnsJd0hFUwf4Z218AsH2f7QO2HwI+zsGul13Awo7dFwD3zlzJERExkV6ulhFwGXC77Q92tM/v2Oxc4NZ6eQOwQtKRkhYDS4BNM1dyRERMpJerZZ4PXADcImlL3fYO4HxJy6i6XO4G3ghge5uk9cBtVFfarM6VMhERc2vCcLf9Lbr3o39lnH3WAGumUVdERExD7lCNiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIK1Ms0ewslfUPS7ZK2SXpL3X68pGsl3Vk/HtexzyWSdkjaLumM2fwHRETEoXo5ct8PvN32bwLPAVZLWgpcDGy0vQTYWK9TP7cCOBU4E7hU0rzZKD4iIrqbMNxt77Z9U728F7gdOBlYDqyrN1sHnFMvLweusL3P9l3ADuC0Ga47IiLGMak+d0mLgGcBNwAn2d4N1R8A4MR6s5OBnR277arbRr/WKklDkoaGh4enUHpERIyl53CXdCzweeCtth8cb9MubT6kwV5re9D24MDAQK9lRERED3oKd0lHUAX7Z2x/oW6+T9L8+vn5wJ66fRewsGP3BcC9M1NuRET0operZQRcBtxu+4MdT20AVtbLK4EvdrSvkHSkpMXAEmDTzJUcERETObyHbZ4PXADcImlL3fYO4H3AekkXAvcA5wHY3iZpPXAb1ZU2q20fmOnCIyKmatHFX266BO5+30tn9fUnDHfb36J7PzrA6WPsswZYM426IiJiGnKHakREgRLuEREFSrhHRBQo4R4RUaCEe0REgRLuEREFSrhHRBQo4R4RUaCEe0REgRLuEREFSrhHRBQo4R4RUaBeRoWMiAK0YSTEOChH7hERBUq4R0QUKOEeEVGgXqbZu1zSHkm3drS9W9IPJW2pf87qeO4SSTskbZd0xmwVHhERY+vlyP2TwJld2j9ke1n98xUASUuBFcCp9T6XSpo3U8VGRERvJgx329cDP+7x9ZYDV9jeZ/suYAdw2jTqi4iIKZhOn/ubJW2tu22Oq9tOBnZ2bLOrbjuEpFWShiQNDQ8PT6OMiIgYbarh/lHgFGAZsBv4QN3ebSJtd3sB22ttD9oeHBgYmGIZERHRzZTC3fZ9tg/Yfgj4OAe7XnYBCzs2XQDcO70SIyJisqYU7pLmd6yeC4xcSbMBWCHpSEmLgSXApumVGBERkzXh8AOSPgu8EDhB0i7gXcALJS2j6nK5G3gjgO1tktYDtwH7gdW2D8xK5RERMaYJw932+V2aLxtn+zXAmukUFRER05M7VCMiCpRwj4goUMI9IqJACfeIiAIl3CMiCpRwj4goUMI9IqJACfeIiAJlguwoWiaFjrbKkXtERIES7hERBUq4R0QUKOEeEVGghHtERIES7hERBZow3OsJsPdIurWj7XhJ10q6s348ruO5SyTtkLRd0hmzVXhERIytlyP3TwJnjmq7GNhoewmwsV5H0lJgBXBqvc+lkubNWLUREdGTXmZiul7SolHNy6mm3gNYB1wH/FHdfoXtfcBdknZQTZ793Rmqd0z9cLMK5IaViOgPU+1zP8n2boD68cS6/WRgZ8d2u+q2iIiYQzN9QlVd2tx1Q2mVpCFJQ8PDwzNcRkREu0013O+TNB+gftxTt+8CFnZstwC4t9sL2F5re9D24MDAwBTLiIiIbqYa7huAlfXySuCLHe0rJB0paTGwBNg0vRIjImKyJjyhKumzVCdPT5C0C3gX8D5gvaQLgXuA8wBsb5O0HrgN2A+stn1glmqPiIgx9HK1zPljPHX6GNuvAdZMp6iIiJie3KEaEVGghHtERIES7hERBUq4R0QUKOEeEVGghHtERIES7hERBUq4R0QUKOEeEVGghHtERIES7hERBUq4R0QUKOEeEVGghHtERIES7hERBUq4R0QUaMLJOsYj6W5gL3AA2G97UNLxwF8Bi4C7gVfZ/sn0yoyIiMmYiSP3F9leZnuwXr8Y2Gh7CbCxXo+IiDk0G90yy4F19fI64JxZ+B0RETGO6Ya7gWskbZa0qm47yfZugPrxxG47SlolaUjS0PDw8DTLiIiITtPqcweeb/teSScC10q6o9cdba8F1gIMDg56mnVERESHaR252763ftwDXAmcBtwnaT5A/bhnukVGRMTkTDncJR0j6dEjy8DvArcCG4CV9WYrgS9Ot8iIiJic6XTLnARcKWnkdf7S9tWSbgTWS7oQuAc4b/plRkTEZEw53G1/H3hml/b7gdOnU1RERExP7lCNiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKNGvhLulMSdsl7ZB08Wz9noiIONSshLukecD/Bv4tsBQ4X9LS2fhdERFxqNk6cj8N2GH7+7Z/CVwBLJ+l3xUREaPI9sy/qPRK4Ezbr6/XLwD+pe03d2yzClhVrz4F2D7jhUzeCcCPmi6iT+S9OCjvxUF5Lw7qh/fiibYHuj0x5QmyJ6AubQ/7K2J7LbB2ln7/lEgasj3YdB39IO/FQXkvDsp7cVC/vxez1S2zC1jYsb4AuHeWfldERIwyW+F+I7BE0mJJjwJWABtm6XdFRMQos9ItY3u/pDcDXwXmAZfb3jYbv2uG9VU3UcPyXhyU9+KgvBcH9fV7MSsnVCMiolm5QzUiokAJ94iIAiXcIyIKlHCPQ0g6pukamibpLb20tYWkoyU9pek6onetD3dJT5T0knr5aEmPbrqmpkh6nqTbgNvr9WdKurThspqyskvba+e6iH4g6WxgC3B1vb5MUmsvbZZ0kqSX1T8nNl3PWFod7pLeAPw18Od10wLgbxorqHkfAs4A7gewfTPwO41WNMcknS/pS8BiSRs6fr5B/b600Lupxot6AMD2FmBRY9U0SNKrgE3AecCrgBvq4Vb6zmwNP/BIsZrqQ3sDgO07+/kv8VywvVN62OgRB5qqpSHfAXZTjRvygY72vcDWRipq3n7bPx31uWirPwZ+2/YeAEkDwNeoDhL7StvDfZ/tX458aCUdzqgxcFpmp6TnAa7vLL6IuoumLWz/APgB8Nyma+kjt0r6PWCepCVUn4vvNFxTUw4bCfba/fRpD0hfFjWHvinpHcDRkv4N8DngSw3X1KQ3UX2bOZlqfKBl9XrrSHqFpDsl/VTSg5L2Snqw6boa8h+BU4F9wGeBB4G3NllQg66W9FVJr5X0WuDLwN82XFNXrb5DVdJhwIXA71KNZPlV4C/c5jclAJC0Azjbdqu+ucTEJL0CeAFVZlxv+8qGS+qq1eEeDydpMdVR2iI6uuxsv7ypmpoi6du2n990Hf1A0pOB/8yhn4sXN1VTUyT9D9t/NFFbP2hluEu6hXH61m0/Yw7L6RuSbgYuA24BHhppt/3NxopqiKQPA4+nunpq30i77S80VVNT6s/Fx4DNdJxgt725saIaIukm288e1ba1HzOjrSdUX9Z0AX3qF7b/V9NF9InHAP9E1WU3wkDrwp3qapmPNl1EkyT9B+APgCdJ6rxq6tHAt5upanytPHIf8Uj6ijUX6isilgDX8PCj1ZsaKyoaI+n4evEiYA9wJQ//XPy4ibqaIOmxwHHAe4GLO57a26/vQ9vD/RHzFWsuSHovcAHwjxzslnFL+1aPojrZfipw1Ei77dc1VtQck3QX1beVrtNm2n7SHJfUN+r7YTo/F/c0WE5XreyWeSR+xZoj5wJPsv3LpgvpA58C7qC6Y/e/Aa+mfdf8L266hn5TD8XwQeAJVN9mnkj1uTi1ybq6aet17n8JnE019d/ZHT+/Zfs1TRbWsJuBxzVdRJ/4Ddt/Avzc9jrgpcDTG66pEZJWS3pcx/pxkv6gwZKa9N+B5wD/UP/xO50+PSBsZbjb/qntu22fTxVmI+G+cNwdy3cScEd9k8avxlVpuqiG/HP9+ICkpwGPpaXjqQBvsP3AyIrtnwBvaK6cRv2z7fuBwyQdZvsbVDf79Z1WdsuMkHQRsIqDV0B8WtJa2x9psKwmvavpAvrIWknHAe+k+oZ3LPAnzZbUmMMkaeTmPknzgEc1XFNTHpB0LHA98BlJe4D9DdfUVdtPqG4Fnmv75/X6McB323pCNUDSAtu7xnjubNutG55C0vupvrV8jOoE65uAnbbf3mRdTagz4hdUJ5lfTfWN7jP10XxfaXu430I1wtsv6vWjgBttt7VvdS8Hb+56FHAEVZ/zY5qram5J2g6cYfvuUe2/D7zT9imNFNagepiOVcBLqELtGqphOto2YugjSqu7ZYBPUI3HPDI2xDnA5c2V0yzbD5uoRNI5VEMit8l/Aq6VdJbtOwEkXQL8HvCvG62sIbYfojpq/1h97fuCtgV7x4HPyGWhIwdBorostO8OgFp95A4g6dk8fBCg7zVcUl+R9Pe2n9N0HXNJ0ulUE7icA7we+G3gZfWJxNaRdB3wcqqDwS3AMPBN229rsKyYQKuP3CV9yvYFwE1d2lqnHu1uxGHAIC0c3972xno41+uoxi0/faTrrqUea/tBSa8HPmH7XaPuDyle3WX7JuA3qCZtudx2X55IHdHqcGfUjQf1VQC/1VAt/eDsjuX9wN3A8mZKacaor99HUl3HvEfVjC59+fV7DhwuaT7VtHJ/3HQxDVlHdXns3wFnUWVHX0+Y3spwr/tQRybpGJmAQcAvgbWNFdYw27/fdA1NG33eIYDqDt2vAt+yfaOkJwF3NlzTXFs6cqGFpMuo5lHta63uc5f0XtuXNF1H0yR9hPGHQL5oDsuJPiNpoe2do9oeb/v/NlXTXBs9DlW3can6TSuP3DtcJekY2z+X9Brg2cCH63k022SoY/k95GameLi7JH0OeJ3t/1e3fYXq/0tbPHPUt/yRb/19213X9iP3rcAzgWdQDRR1GfAK26285A1A0vdsP6vpOqJ/SPoe8HGqUTJfZfsf8znpf60cW6bD/vqW6uVUR+wfphoZss3a+9c+xmLbl1KN6/6lemTEfE76XNu7ZfbWJ1dfA/xOfbXMEQ3XFNFvBGD72/U9AH8FPLXZkmIibe+WeTzVnYc32v47Sb8OvND2/2m4tDk1atiBX6OaXg76uD8x5o6k59n+Tsf64cDzbF/fYFkxgVaHe0RMbIwZyzbbbvM9IX2vld0ykr5l+wWjjlghR6oRvyLpqVQ36zx21N3Lj6FjirnoT60Md9svqB/bfvI0YjxPAV7GwQltRuylvZN1PGK0tlumHsZ0q+2nNV1LRD+T9Fzb3226jpic1l4KWQ9jenN9EjUixna/pI2SbgWQ9AxJ72y6qBhfa4/cASR9nWo4103Az+tm227VYFkR45H0TeAPgT8fuXFJ0q351tvfWtnn3uE9HcuiGtf9/IZqiehXv2Z7UzUw5q/09XC30eJuGQDb3wR+CrwU+CTV8K4fa7KmiD70I0mnUF9ZJumVwO5mS4qJtPLIXdKTgRVUR+n3U91xJ9svarSwiP60mmoo7KdK+iFwF9Vd3dHHWtnnLukhqkH3L7S9o277vu0nNVtZRP+SdAxwmO29TdcSE2vlkTvw76iO3L8h6WrgCg5OfBsRgKTX2P60pLeNagfA9gcbKSx60spwt30lcGV9JHIO1Yz3J0n6KHCl7WuarC+iTxxTP+Zmv0egVnbLdCPpeOA84N/bfnHT9URETEfCPSK6kvRfx3natv90zoqJSUu4R0RXkt7epfkYqhmZ/oXtY+e4pJiEhHtETEjSo4G3UAX7euADtvc0W1WMp5UnVCOiN/W5qLcBrwbWAc+2/ZNmq4peJNwjoitJ7wdeQXUD09Nt/6zhkmIS0i0TEV3VN/vtoxpHJpPaPMIk3CMiCtTqgcMiIkqVcI+IKFDCPWIUSedIWtqxfp2kwSZripishHvEoc4Blk60US8k5Yq0aETCPVpB0t9I2ixpm6RVddvPOp5/paRPSnoe8HLg/ZK21JNUAJwnaZOkf5D0r+p9jpL0CUm3SPqepBfV7a+V9DlJXwIyCF00IkcV0Ravs/1jSUcDN0r6fLeNbH9H0gbgKtt/Db8a4vZw26dJOgt4F/ASqkkssP10SU8FrqknggF4LvAM2z+e3X9WRHcJ92iLiySdWy8vBJZMcv8v1I+bgUX18guAjwDYvkPSD4CRcL82wR5NSrhH8SS9kOpI+7m2/0nSdcBRPPzGnKMmeJl99eMBDv6/GW+Cl59PutCIGZQ+92iDxwI/qYP9qcBz6vb7JP2mpMOAczu230tvE1RcTzXmysi8vL8ObJ+5siOmLuEebXA1cLikrcCfAn9ft18MXAV8Hdjdsf0VwB/WJ0lPYWyXAvMk3UI1yfprbe8bZ/uIOZPhByIiCpQj94iIAiXcIyIKlHCPiChQwj0iokAJ94iIAiXcIyIKlHCPiChQwj0iokD/Hyo/n6pHXUk/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "author\n",
       "Aristotle    3\n",
       "Hume         1\n",
       "Kant         0\n",
       "Nietzsche    4\n",
       "Plato        2\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nan = philo_df.isna()\n",
    "print(df_nan.sum())\n",
    "\n",
    "# print(philo_df.describe())\n",
    "\n",
    "philo_df['word_count'] = philo_df['sentence'].apply(lambda x: x.count(' '))\n",
    "\n",
    "\n",
    "philo_df.groupby('author')['word_count'].mean().plot.bar()\n",
    "plt.show()\n",
    "philo_df.groupby('author')['label'].count().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "philo_df.groupby('author')['label'].size().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "philo_df.groupby('author')['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.80\n",
    "train_valid_ratio = 0.80\n",
    "df_train, df_test = train_test_split(philo_df, train_size = train_test_ratio, random_state = ran_state)\n",
    "df_train, df_val = train_test_split(philo_df, train_size = train_valid_ratio, random_state = ran_state)\n",
    "\n",
    "# Write preprocessed data\n",
    "df_train.to_csv(dl_folder + '/train.csv', index=False)\n",
    "df_val.to_csv(dl_folder + '/val.csv', index=False)\n",
    "df_test.to_csv(dl_folder + '/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "\n",
    "# Models\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bettyld/anaconda3/envs/py37/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/bettyld/anaconda3/envs/py37/lib/python3.7/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/bettyld/anaconda3/envs/py37/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First instances for training {text, label} {'sentence': ['When', 'he', 'had', 'bathed', ',', 'and', 'his', 'children', 'were', 'brought', 'to', 'him', ',', 'and', 'the', 'women', 'belonging', 'to', 'his', 'family', 'were', 'come', ',', 'having', 'conversed', 'with', 'them', 'in', 'the', 'presence', 'of', 'Crito', ',', 'and', 'given', 'them', 'such', 'injunctions', 'as', 'he', 'wished', ',', 'he', 'directed', 'the', 'women', 'and', 'children', 'to', 'go', 'away', ',', 'and', 'then', 'returned', 'to', 'us', '.', ', having', 'then returned', 'family were', 'were brought', 'come ,', 'them in', 'and then', 'brought to', 'the women', 'he wished', 'he directed', 'returned to', 'to his', 'away ,', ', and', 'his children', 'to go', 'bathed ,', 'given them', 'us .', 'women and', 'go away', 'children were', 'children to', 'women belonging', 'conversed with', 'and children', 'and the', 'with them', 'them such', 'to us', ', he', 'he had', 'were come', 'Crito ,', 'and given', 'directed the', 'such injunctions', 'as he', 'his family', 'belonging to', 'When he', 'in the', 'having conversed', 'him ,', 'the presence', 'of Crito', 'presence of', 'to him', 'injunctions as', 'and his', 'had bathed', 'wished ,'], 'author': '2'}\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def generate_bigrams(seq):\n",
    "    \"\"\"\n",
    "    Add bigrams after 1-gram tokens.\n",
    "    >>> generate_bigrams(['This', 'film', 'is', 'terrible'])\n",
    "    >>> ['This', 'film', 'is', 'terrible', 'film is', 'is terrible', 'This film']\n",
    "    \"\"\"\n",
    "    n_grams = set(zip(*[seq[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        seq.append(' '.join(n_gram))\n",
    "    return seq\n",
    "\n",
    "# Create Fields\n",
    "LABEL = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.long)\n",
    "TEXT = Field(sequential=True, tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm',\n",
    "                  preprocessing = generate_bigrams, include_lengths=True, batch_first=True)\n",
    "fields = [('sentence', TEXT), ('author', LABEL)]\n",
    "\n",
    "# Create TabularDataset\n",
    "train_data, valid_data, test_data = TabularDataset.splits(path=dl_folder, train='train.csv', validation='val.csv', test='test.csv',\n",
    "                                           format='CSV', fields=fields, skip_header=True)\n",
    "\n",
    "print('First instances for training {text, label}', vars(train_data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "# text_field.build_vocab(train, min_freq=3)\n",
    "\n",
    "# We build the vocab with glove 6b embedding\n",
    "# It generates embedding from string\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8361,  0.0741,  0.4035,  0.9052,  0.9093, -0.9562,  0.4209, -3.0193,\n",
       "        -0.4904,  0.8428, -1.3858,  0.3815,  0.3563, -0.2995, -0.6272, -1.1421,\n",
       "         1.3801,  1.9051, -1.6055,  0.9271,  0.0549,  0.3111, -1.0512, -1.4214,\n",
       "         1.4088,  0.7820, -1.0420,  0.3987, -1.4779,  0.4008,  0.8786,  0.4321,\n",
       "        -1.7822,  0.6221, -0.0068, -0.7846,  0.2271,  0.0166, -0.3770,  0.5286,\n",
       "        -0.2253,  0.4584,  0.0371, -0.3824, -1.1905, -0.2926,  1.9259, -0.6985,\n",
       "        -0.3428,  0.9361, -0.4678,  1.5222,  0.4198, -1.1068,  0.4686, -0.0953,\n",
       "        -1.2238,  0.4531, -0.4026,  0.7346,  1.6088,  0.6045,  1.7433, -1.8400,\n",
       "        -0.1446, -0.0592,  0.7178, -0.0487,  0.8786,  1.9768,  0.1268, -0.1412,\n",
       "         0.7519,  1.0235,  1.8326,  0.1836, -1.4087, -0.7043, -0.0641, -0.7196,\n",
       "         0.1921, -1.3827, -0.3775, -0.9951,  0.3862,  0.0821, -0.2431, -1.2590,\n",
       "         0.9977,  0.7609,  2.3067,  0.8580,  0.8785, -1.7842, -1.4248,  1.4945,\n",
       "         0.6284, -0.3802, -0.0185,  1.0531])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To do try animal and city name\n",
    "# Retrieve the vector for dog\n",
    "TEXT.vocab.vectors[TEXT.vocab.stoi['Paris']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "tensor([-1.6614,  1.1545, -0.9539,  1.5112,  1.2421, -1.2255, -0.3273, -0.7928,\n",
      "         0.1389,  1.0507,  0.6977, -0.9729,  0.3348,  0.3781, -0.2271, -0.9677,\n",
      "         0.4873,  0.5258,  0.7060,  0.7559, -1.6422,  1.2089,  1.7998, -0.3703,\n",
      "        -0.7985, -0.3633, -0.5790,  0.7280, -0.3867, -0.9491, -0.3187,  1.1952,\n",
      "        -0.0409,  1.0870, -0.1299, -0.6890,  0.6349,  1.8065,  0.5650,  0.4353,\n",
      "         0.4575,  1.2969, -0.4133,  0.3201, -0.8715,  1.2667, -0.4072, -1.2192,\n",
      "        -1.1033, -0.0458, -0.1512,  0.3465,  0.9425,  0.2403, -1.3882, -1.0304,\n",
      "         0.2646, -0.2421,  0.6515,  0.9840,  0.1514, -2.3562, -0.4326,  1.2781,\n",
      "         0.9605,  0.3671, -0.9989, -0.2501, -0.5505, -0.5285,  0.5271, -1.3253,\n",
      "         0.6040,  1.4566, -1.1941,  0.5740,  1.2820, -0.8345,  0.6322,  0.0383,\n",
      "         1.2307,  0.3444, -1.8191, -0.3241, -0.6964, -0.0823,  0.6648, -0.9254,\n",
      "        -0.3084,  0.6984,  0.0979,  0.3449,  0.7896,  0.2754, -0.9191, -1.0275,\n",
      "        -2.2898,  1.2764,  0.0129, -1.3250])\n"
     ]
    }
   ],
   "source": [
    "# Display frequency, wordindex and \n",
    "# vector for each word respectively of the \n",
    "print(TEXT.vocab.freqs[1])\n",
    "# stoi gives access to a dictionnary containing word and their indexes\n",
    "print(TEXT.vocab.stoi[1])\n",
    "print(TEXT.vocab.vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bettyld/anaconda3/envs/py37/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/bettyld/anaconda3/envs/py37/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4d4609ee5f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next batch instance according to their respective field'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create Iterators\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    sort_key=lambda x: len(x.sentence),\n",
    "    sort=False,\n",
    "    shuffle=True,\n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)\n",
    "\n",
    "batch = next(iter(train_iterator))\n",
    "print('next batch instance according to their respective field', batch, batch.sentence, batch.label)\n",
    "for data in valid_iterator:\n",
    "    (x, x_len), y = data.sentence, data.label\n",
    "    print(x, x_len, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training \n",
    "# Save and Load Functions\n",
    "\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_metrics(load_path):\n",
    "\n",
    "    if load_path==None:\n",
    "        return\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "#     def __init__(self, dimension=128):\n",
    "    def __init__(self, vocab_size,\n",
    "                 embedding_dim, hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional,\n",
    "                 dropout,\n",
    "                 pad_idx):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,\n",
    "                                      padding_idx = pad_idx)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.hidden_size]\n",
    "        out_reverse = output[:, 0, self.hidden_size:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # first layer is the embedded layer that uses 100 length vectors to represent each word.\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = len(LABEL.vocab) # To check\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LSTM(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [-0.6971, -0.8865,  0.7625,  ..., -0.2578, -2.3295, -0.9226],\n",
      "        [ 0.5935, -1.2607, -1.2701,  ..., -2.2648,  1.4020,  2.2614],\n",
      "        [-0.8253, -0.2917, -0.4031,  ..., -0.7828, -0.1828, -1.0616]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import tqdm \n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def fit(model, it, is_eval=False, optim=None, crit=None):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_prec = 0; epoch_rec = 0; epoch_f1 = epoch_prec\n",
    "    if is_eval:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "    for batch in tqdm.tqdm(it, total=len(it)):\n",
    "        if not is_eval:\n",
    "            optim.zero_grad()\n",
    "        text, text_lenghts = batch.sentence\n",
    "        text = text.type(torch.LongTensor)\n",
    "        is_null = text_lenghts==0\n",
    "        if True in is_null: # TO FIX\n",
    "            print(text, text_lenghts)\n",
    "            continue\n",
    "        preds = model(text, text_lenghts)\n",
    "        loss = crit(preds, batch.label)\n",
    "        acc = accuracy_score(batch.label.cpu().detach().numpy(), np.argmax(preds.cpu().detach().numpy(), axis=1))\n",
    "        if not is_eval:\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        else:\n",
    "            prec, recall, f1, _ = precision_recall_fscore_support(batch.label.cpu().detach().numpy(), np.argmax(preds.cpu().detach().numpy(), axis=1), average='weighted')\n",
    "            epoch_prec += prec\n",
    "            epoch_rec += recall\n",
    "            epoch_f1 += f1\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(it), epoch_acc / len(it), epoch_prec / len(it), epoch_rec /len(it), epoch_f1 / len(it)\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion =  nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-7bb336cec19c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m def train(model,\n\u001b[1;32m      2\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iter' is not defined"
     ]
    }
   ],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          train_loader = train_iter,\n",
    "          valid_loader = valid_iter,\n",
    "          num_epochs = 10,\n",
    "          eval_every = len(train_iter) // 2,\n",
    "          file_path = destination_folder,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    # initialize running values\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    # training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc, _ = fit(model, train_iterator, is_eval=False,\n",
    "                                    optim=optimizer, crit=criterion)\n",
    "        valid_loss, valid_acc, val_prec, val_rec, val_f1 = fit(model, valid_iterator, is_eval=True,\n",
    "                                         crit=criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "       \n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Pre: {val_prec:.3f} |  Val. Rec: {val_rec*100:.2f}% | Val. F1: {val_f1*100:.2f}%')\n",
    "\n",
    "    # Last model\n",
    "    torch.save(model.state_dict(), 'tut2-model.pt')\n",
    "    print('Finished Training!')\n",
    "\n",
    "\n",
    "# train(model=model, optimizer=optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'tut2-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 335/335 [01:06<00:00,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.630 | Test Acc: 39.30%\n",
      "Test prec: 0.370 | Test rec: 39.30% | Test f1: 35.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "model.load_state_dict(torch.load('tut2-model.pt'))\n",
    "test_loss, test_acc, test_prec, test_rec, test_f1 = fit(model, test_iterator, crit=criterion, is_eval=True)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "print(f'Test prec: {test_prec:.3f} | Test rec: {test_rec*100:.2f}% | Test f1: {test_f1*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difficulti on thi head must encreas when we consid that our judgment alter veri sensibl accord to the subject and that the same power and proxim will be deem possess in one case which is not esteem such in anoth \n",
      " Hume\n",
      "Prediction: Aristotle\n"
     ]
    }
   ],
   "source": [
    "# glove embed\n",
    "# from torchtext.vocab import GloVe\n",
    "# embedding_glove = GloVe(name='6B', dim=100)\n",
    "import random \n",
    "\n",
    "def generate_sentence(n):\n",
    "    sample = philo_df[['sentence', 'author']].iloc[n%len(philo_df)]\n",
    "    return sample.sentence, sample.author\n",
    "\n",
    "def predict(model, sentence):\n",
    "    model.eval()\n",
    "    # split one sentence into list of words (smartly splitted by spacy)\n",
    "    tokenized = generate_bigrams([tok.text for tok in spacy_en.tokenizer(sentence)])\n",
    "    # Embedded with Glove\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    # Prepare input\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1).transpose(0,1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    out = model(tensor, length_tensor)\n",
    "    out = np.argmax(out.cpu().detach().numpy(), axis=1)[0]\n",
    "    author_label = LABEL.vocab.stoi[str(out)]\n",
    "    author_label = enc_aut[author_label]\n",
    "    return author_label\n",
    "sentence_ex, label_ex = generate_sentence(random.randint(0, len(philo_df)))\n",
    "print(sentence_ex, '\\n', label_ex)\n",
    "print('Prediction:', predict(model, sentence_ex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds =  [[0.48278892, 0.4991066,  0.5182273,  0.50137734, 0.5032032,  0.50438505],\n",
    "[0.48278892, 0.4991066,  0.5182273,  0.50137734, 0.5032032,  0.50438505]]\n",
    "np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    i shall therefor ventur to acknowledg that not...\n",
       "author                                                   Hume\n",
       "Name: 23620, dtype: object"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "philo_df[['sentence', 'author']].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fdebe9f5160>>, {'<unk>': 0, '1': 1, '2': 2, '4': 3, '0': 4, '3': 5, 1: 0, 3: 0, '[4]': 0, '[1]': 0}) {1: 'Hume', 2: 'Plato', 4: 'Nietzsche', 3: 'Aristotle', 0: 'Kant'}\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi, enc_aut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
