{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_state = 42\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_dir = f'{current_dir}/dataset'\n",
    "dl_folder = f'{current_dir}/dataset/dl'\n",
    "destination_folder = f'{current_dir}/results'\n",
    "filenames = [\n",
    "    'kant.txt', \n",
    "    'aristotle.txt', \n",
    "    'plato.txt', \n",
    "    'hume.txt',\n",
    "    'nietzsche.txt'\n",
    "    ]\n",
    "\n",
    "[os.path.join(data_dir, file) for file in filenames]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'sentences.csv'\n",
    "data_csv = os.path.join(data_dir, csv_file)\n",
    "philo_df = pd.read_csv(data_csv).sample(frac = 1)\n",
    "# philo_df = philo_df.loc[philo_df.author=='Nietzsche']\n",
    "philo_all_df = philo_df.copy()\n",
    "philo_df = philo_df.iloc[:len(philo_df)//3]\n",
    "philo_df = philo_df.reset_index()\n",
    "philo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan = philo_df.isna()\n",
    "# print(df_nan.sum())\n",
    "\n",
    "# print(philo_df.describe())\n",
    "\n",
    "philo_df['word_counter'] = philo_df['sentence'].apply(lambda x: x.count(' '))\n",
    "\n",
    "\n",
    "philo_df.groupby('author')['word_count'].mean().plot.bar()\n",
    "plt.show()\n",
    "philo_df.groupby('author')['label'].count().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "philo_df.groupby('author')['label'].size().plot.bar()\n",
    "plt.show()\n",
    "\n",
    "philo_df.groupby('author')['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "first_n_words = 200\n",
    "\n",
    "def trim_string(x):\n",
    "    x = x.split(maxsplit=first_n_words)\n",
    "    x = ' '.join(x[:first_n_words])\n",
    "    return x\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "PATTERN_S = re.compile(\"\\'s\")\n",
    "PATTERN_RN = re.compile(\"\\\\r\\\\n\")\n",
    "PATTERN_PUNC = re.compile(r\"[^\\w\\s]\")\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "MIN_WORDS = 2\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        # TODO What is doing spacy\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub(PATTERN_S, ' ', text)\n",
    "    text = re.sub(PATTERN_RN, ' ', text)\n",
    "    text = re.sub(PATTERN_PUNC, ' ', text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', ' ')\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    tokens = [w for w in text.split() if not w in STOPWORDS] # remove stopwors from text\n",
    "    # Remove short words (under 3 characters) from the tokens\n",
    "    long_words = []\n",
    "    for token in tokens:\n",
    "        if len(token) >= MIN_WORDS:\n",
    "            long_words.append(token)\n",
    "    # Join the tokens back together\n",
    "    cleaned_text = (\" \".join(long_words)).strip()\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "# for tokenizer\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Processing the data\n",
    "# Drop empty text\n",
    "philo_df.drop(philo_df[philo_df.sentence.str.len()<3].index, inplace=True)\n",
    "\n",
    "# To try\n",
    "philo_df['clean_sentence'] = philo_df['sentence'].apply(clean_text)\n",
    "philo_df['sentence'] = philo_df['clean_sentence'].apply(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "\n",
    "# trim \n",
    "# philo_df['trim_sentence'] = philo_df['sentence'].apply(trim_string)\n",
    "\n",
    "# tokenized\n",
    "philo_df['token_sentence'] = philo_df['clean_sentence'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# # To try\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer() #PorterStemmer()\n",
    "# philo_df[\"token_sentence\"] = philo_df['token_sentence'].str.split().apply(lambda x: ' '.join([stemmer.stem(w.lower()) for w in x]))\n",
    "philo_df[\"token_sentence\"] = philo_df['token_sentence'].apply(lambda x: [stemmer.lemmatize(w) for w in x])\n",
    "\n",
    "mini_philo_df = philo_df[['sentence', 'label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "philo_df['spacy_sentence'] = philo_df['sentence'].apply(lambda x: nlp(x.lower())) # calling nlp on a string and spaCy tokenizes the text and creates a document object\n",
    "# philo_df['spacy_sentence_token'] = philo_df['sentence'].apply(lambda x: nlp(x.lower()).text.split()) # calling nlp on a string and spaCy tokenizes the text and creates a document object\n",
    "\n",
    "# philo_all_df['spacy_sentence'] = philo_all_df['sentence'].apply(lambda x: nlp(x.lower())) # calling nlp on a string and spaCy tokenizes the text and creates a document object\n",
    "philo_df['spacy_vec'] = philo_df['spacy_sentence'].apply(lambda x: np.array(x.vector))\n",
    "philo_df['spacy_vec'] = philo_df['spacy_vec'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "def decompose_embed(X):\n",
    "    pca = PCA(n_components=2)\n",
    "    y = pca.fit_transform(X)\n",
    "    return y\n",
    "    \n",
    "\n",
    "def show_embedding(X, aut, sentence):\n",
    "    fig = px.scatter(x=y[:,0], y=y[:,1], color=aut)\n",
    "    fig.update_layout(title='Word embedding')\n",
    "\n",
    "    fig.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "\n",
    "# Version 1\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Using TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english') #, ngram_range=(1, 2)) #one gram to three gram\n",
    "tfidf_mat = vectorizer.fit_transform(philo_df['sentence'].values)\n",
    "# Compute cosine similarity\n",
    "cosine_sim_mat = cosine_similarity(tfidf_mat, tfidf_mat)\n",
    "# cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(tfidf_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = philo_df.sample(n=100, random_state=42)\n",
    "sub_df.spacy_vec = sub_df.spacy_vec.apply(lambda x: np.array(x))\n",
    "X = np.array([np.array(e) for e in sub_df.spacy_vec.values])\n",
    "y = decompose_embed(X)\n",
    "aut_list = sub_df.author.values\n",
    "sent_list = sub_df.sentence.values\n",
    "print(aut_list)\n",
    "show_embedding(y, aut_list, sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 tfidf recommandation matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tfidf_mat.shape) \n",
    "# print(cosine_sim_mat.shape)\n",
    "\n",
    "def get_recommendations_v1(sentence, series):\n",
    "    tokens = [str(tok) for tok in nlp(sentence)]\n",
    "    # using tfidf\n",
    "    vec = vectorizer.transform(tokens)\n",
    "    # using spacy\n",
    "#     print('computing similarity')\n",
    "    mat = cosine_similarity(vec, tfidf_mat)\n",
    "    best_sim_each_token = np.argmax(mat, axis=1)\n",
    "    index = np.argsort(best_sim_each_token)[::-1] #take the five highest norm \n",
    "#     print('norms, indices', best_sim_each_token, index)\n",
    "    null_index = best_sim_each_token != 0\n",
    "    null_index = null_index[index]\n",
    "    index = index[null_index==True]\n",
    "    best_index = best_sim_each_token[index][:3]\n",
    "#     print('best_index', best_index)\n",
    "    print(philo_df[['sentence', 'author']].iloc[best_index])\n",
    "    return best_index\n",
    "\n",
    "\n",
    "mat = get_recommendations_v1('Can I eat a hot soup tonight?', philo_df['sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 spacy vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_v2(sentence, series):\n",
    "    vec = np.array([tok.vector for tok in nlp(sentence)])\n",
    "    print('computing similarity')\n",
    "    data_vec = np.array([np.array(elt) for elt in philo_df['spacy_vec'].values])\n",
    "    print(vec.shape, data_vec.shape)\n",
    "    mat = cosine_similarity(vec, data_vec)\n",
    "    best_sim_each_token = np.argmax(mat, axis=1)\n",
    "    index = np.argsort(best_sim_each_token)[::-1] #take the five highest norm \n",
    "    print('norms, indices', best_sim_each_token, index)\n",
    "    null_index = best_sim_each_token != 0\n",
    "    null_index = null_index[index]\n",
    "    index = index[null_index==True]\n",
    "    best_index = best_sim_each_token[index][:3]\n",
    "    print('best_index', best_index)\n",
    "    print(philo_df[['sentence', 'author']].iloc[best_index])\n",
    "    return best_index\n",
    "\n",
    "\n",
    "mat = get_recommendations_v2('Can I eat a hot soup tonight?', philo_df['sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version #3 with word2vec gensim\n",
    "# missing keys (common words)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader\n",
    "import gensim.downloader as api\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "# 'glove-wiki-gigaword-300'\n",
    "\n",
    "# glove_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "glove_vectors = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=philo_df.spacy_sentence.values, vector_size=300, window=5, min_count = 1, workers = 2)\n",
    "word2vec_model.build_vocab(philo_df.sentence.values)\n",
    "print(word2vec_model.wv)\n",
    "# word2vec_model.intersect_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', lockf=0.0,binary=True)\n",
    "word2vec_model.train(philo_df.sentence.values, total_examples=2, epochs = 2)\n",
    "print(word2vec_model.wv)\n",
    "word2vec_model.save('my_gensim_word2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access vectors for specific words with a keyed lookup:\n",
    "# philo_df['gensim_vec'] = philo_df['spacy_sentence'].apply(lambda x: [word2vec_model.wv[elt.text] for elt in x])\n",
    "print(philo_df.spacy_sentence.values)\n",
    "print(word2vec_model.wv.key_to_index)\n",
    "# word2vec_model = model.wv.get_vecattr(\"rock\", \"count\")  # 👍\n",
    "# word2vec_model = len(model.wv)  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_v3(sentence, series):\n",
    "    vec = [model[str(tok)] for tok in nlp(sentence)]\n",
    "    print('computing similarity')\n",
    "    mat = cosine_similarity(vec, philo_df['spacy_vec'].values)\n",
    "    best_sim_each_token = np.argmax(mat, axis=1)\n",
    "    index = np.argsort(best_sim_each_token)[::-1] #take the five highest norm \n",
    "    print('norms, indices', best_sim_each_token, index)\n",
    "    null_index = best_sim_each_token != 0\n",
    "    null_index = null_index[index]\n",
    "    index = index[null_index==True]\n",
    "    best_index = best_sim_each_token[index][:3]\n",
    "    print('best_index', best_index)\n",
    "    print(philo_df[['sentence', 'author']].iloc[best_index])\n",
    "    return best_index\n",
    "\n",
    "mat = get_recommendations_v3('I can\\'t wait seeing you again', philo_df['sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 4 Doc2Vec\n",
    "# import\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(philo_df['spacy_sentence'].values)]\n",
    "\n",
    "# Training Doc2Vec\n",
    "## Train doc2vec model\n",
    "model = Doc2Vec(tagged_data, vector_size = 20, window = 2, min_count = 1, epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagged_data[0])\n",
    "print(model.docvecs)\n",
    "\n",
    "def get_recommendations_v4(sentence, series):\n",
    "    print(sentence.split())\n",
    "    vec = model.infer_vector(sentence.split())\n",
    "    print('computing similarity')\n",
    "    results = model.docvecs.most_similar(positive = [vec])\n",
    "    best_idx, score = list(zip(*results))\n",
    "    print(list(best_idx))\n",
    "    print(philo_df[['sentence', 'author']].iloc[list(best_idx[:3])])\n",
    "    \n",
    "get_recommendations_v4('what is virtue?', philo_df['sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 5\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "philo_df['bert_vec'] = philo_df['spacy_sentence'].apply(lambda x: sbert_model.encode(x.text))\n",
    "\n",
    "def get_recommendations_v4(sentence, series):\n",
    "    vec = model.encode([sentence])[0]\n",
    "    print('computing similarity')\n",
    "    mat = cosine_similarity(vec, philo_df['bert_vec'].values)\n",
    "    best_sim_each_token = np.argmax(mat, axis=1)\n",
    "    index = np.argsort(best_sim_each_token)[::-1] #take the five highest norm \n",
    "    print('norms, indices', best_sim_each_token, index)\n",
    "    null_index = best_sim_each_token != 0\n",
    "    null_index = null_index[index]\n",
    "    index = index[null_index==True]\n",
    "    best_index = best_sim_each_token[index][:3]\n",
    "    print('best_index', best_index)\n",
    "    print(philo_df[['sentence', 'author']].iloc[best_index])\n",
    "    return best_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6 Torch bert transformers\n",
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')    # Download vocabulary from S3 and cache.\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')    # Download model and configuration from S3 and cache.\n",
    "# model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased', output_attentions=True)  # Update configuration during loading\n",
    "# assert model.config.output_attentions == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = \"Who was Jim Henson ?\"\n",
    "text_2 = \"Jim Henson was a puppeteer\"\n",
    "indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 6 #infersen\n",
    "# thank you https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir encoder\n",
    "! curl -Lo encoder/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\n",
    "  \n",
    "! mkdir GloVe\n",
    "! curl -Lo GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
    "! unzip GloVe/glove.840B.300d.zip -d GloVe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import InferSent\n",
    "import torch\n",
    "\n",
    "V = 2\n",
    "MODEL_PATH = 'encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "model_infersen = InferSent(params_model)\n",
    "model_infersen.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "W2V_PATH = '/content/GloVe/glove.840B.300d.txt'\n",
    "model_infersen.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_infersen.build_vocab(philo_df['spacy_sentence'].values, tokenize=True)\n",
    "infersen_mat = np.array([model.encode([sent])[0] for sent in philo_df['spacy_sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_v4(sentence, series):\n",
    "    vec = model_infersen.encode([sentence])[0]\n",
    "    print('computing similarity')\n",
    "    mat = cosine_similarity(vec, infersen_mat)\n",
    "    best_sim_each_token = np.argmax(mat, axis=1)\n",
    "    index = np.argsort(best_sim_each_token)[::-1] #take the five highest norm \n",
    "    print('norms, indices', best_sim_each_token, index)\n",
    "    null_index = best_sim_each_token != 0\n",
    "    null_index = null_index[index]\n",
    "    index = index[null_index==True]\n",
    "    best_index = best_sim_each_token[index][:3]\n",
    "    print('best_index', best_index)\n",
    "    print(philo_df[['sentence', 'author']].iloc[best_index])\n",
    "    return best_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
