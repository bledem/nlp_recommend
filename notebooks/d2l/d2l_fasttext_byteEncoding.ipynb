{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667af162",
   "metadata": {},
   "source": [
    "# Fast text and subwords embeddings\n",
    "\n",
    "This notebook explore the embeddings of words according to their sub-words structure.\n",
    "Subwords models capture the similarity of words according to their roots and composition.\n",
    "In that sense, ``theology`` will be close to ``geology``.\n",
    "\n",
    "## More about..\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-auto-examples-tutorials-run-fasttext-py :\n",
    "\n",
    "The main principle behind fastText is that the morphological structure of a word carries important information about the meaning of the word. Such structure is not taken into account by traditional word embeddings like Word2Vec, which train a unique word embedding for every individual word.\n",
    "\n",
    "### FastText\n",
    "fastText attempts to solve this by treating each word as the aggregation of its subwords. For the sake of simplicity and language-independence, subwords are taken to be the character ngrams of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.\n",
    "\n",
    "### FastText or Word2Vec?\n",
    "- Word2vec > FasText on semantic tasks\n",
    "- TasText > Word2Vec on syntactic tasks\n",
    "The difference goes smaller as the size of the training set increases.\n",
    "\n",
    "### Main advantage:\n",
    "fastText can obtain vectors even for out-of-vocabulary (OOV) words, by summing up vectors for its component char-ngrams, provided at least one of the char-ngrams was present in the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b352ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fdad53",
   "metadata": {},
   "source": [
    "# Load english model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab5b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bettyld/.local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁this', '▁is', '▁an', 'arch', 'ism']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpemb_en = BPEmb(lang=\"en\")\n",
    "bpemb_en.encode(\"Stratford\")\n",
    "bpemb_en.encode(\"This is anarchism\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25030e6a",
   "metadata": {},
   "source": [
    "# Get embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da5c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(type(bpemb_en.emb))\n",
    "print(bpemb_en.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f638b",
   "metadata": {},
   "source": [
    "# find unknown similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913fdbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁disease', 0.8304932117462158),\n",
       " ('▁diagn', 0.8269731998443604),\n",
       " ('itis', 0.7845749855041504),\n",
       " ('▁patients', 0.7690582871437073),\n",
       " ('▁cancer', 0.7671700119972229),\n",
       " ('▁symptoms', 0.7551704049110413),\n",
       " ('orders', 0.7446448802947998),\n",
       " ('▁syndrome', 0.7400376796722412),\n",
       " ('▁hyp', 0.7319927215576172),\n",
       " ('▁treatment', 0.7303920388221741),\n",
       " ('▁diseases', 0.7287130951881409),\n",
       " ('▁chronic', 0.7265974283218384),\n",
       " ('ysis', 0.7166935205459595),\n",
       " ('▁tum', 0.7108365297317505),\n",
       " ('▁neuro', 0.708076000213623),\n",
       " ('▁inf', 0.7016445994377136),\n",
       " ('▁sympt', 0.6954596638679504),\n",
       " ('▁disorder', 0.6911265254020691),\n",
       " ('inal', 0.6867300271987915),\n",
       " ('▁surgery', 0.6753949522972107)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpemb_en.most_similar(\"osis\", topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59f4ea",
   "metadata": {},
   "source": [
    "# Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d57717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts  # some example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2524dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'interface', 'computer']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36, 290)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(common_texts[0])\n",
    "model = FastText(vector_size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(common_texts)\n",
    "model.train(corpus_iterable=common_texts, total_examples=len(common_texts), epochs=10)  # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7edc8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FastText(vector_size=4, window=3, min_count=1, sentences=common_texts, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79a49a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.allclose(model.wv['computer'], model2.wv['computer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0ff49",
   "metadata": {},
   "source": [
    "# Create embedding for non-existent words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4caf1c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "[-0.00263242 -0.03817195 -0.02007434  0.00741351] [-0.02574481 -0.01075656  0.00200511  0.03659087]\n"
     ]
    }
   ],
   "source": [
    "# existent word\n",
    "existent_word = \"computer\"\n",
    "print(existent_word in model.wv.key_to_index)\n",
    "computer_vec = model.wv[existent_word]  # numpy vector of a word\n",
    "\n",
    "# Non-existent word\n",
    "oov_word = \"graph-out-of-vocab\"\n",
    "print(oov_word in model.wv.key_to_index)\n",
    "oov_vec = model.wv[oov_word]  # numpy vector for OOV word\n",
    "\n",
    "print(computer_vec, oov_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebf907",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding\n",
    "\n",
    "\n",
    "Byte pair encoding performs a statistical analysis of the training dataset to discover common symbols within a word, such as consecutive characters of arbitrary length. Starting from symbols of length  1 , byte pair encoding iteratively merges the most frequent pair of consecutive symbols to produce new longer symbols. \n",
    "\n",
    "[sce](https://d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7ca1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = [\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o',\n",
    "    'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1cce51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with single character frequency\n",
    "\n",
    "raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "token_freqs = {}\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4b7d0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs ends: defaultdict(<class 'int'>, {('f', 'a'): 7, ('a', 's'): 7, ('s', 't'): 7, ('t', '_'): 4, ('t', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('t', 'a'): 9, ('a', 'l'): 9, ('l', 'l'): 9, ('l', '_'): 5, ('l', 'e'): 4})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('t', 'a')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    \"\"\"Returns the most frequent pair of consecutive\n",
    "    symbols within a word,\n",
    "    where words are the keys of argument ``token_freqs``\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key of `pairs` is a tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    print('pairs ends:', pairs)\n",
    "    return max(pairs, key=pairs.get)  # Key of `pairs` with the max value\n",
    "max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "max_freq_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7a2d79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 'ta l l _': 5, 'ta l l e r _': 4}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we merge the characters with the highest frequency as the same character\n",
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    \"\"\"merge the most frequent pair of consecutive symbols\n",
    "    to produce new symbols\"\"\"\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs\n",
    "merge_symbols(max_freq_pair, token_freqs, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf29bd5",
   "metadata": {},
   "source": [
    "### Process byte pair encoding iteratively.\n",
    "\n",
    "First iteration the most frequent pair of consecutive symbols are 't' and 'a'.\n",
    "\n",
    "Second iteration, the most frequent pair of consecutive symbols are 'f' and 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05771829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs ends: defaultdict(<class 'int'>, {('f', 'a'): 7, ('a', 's'): 7, ('s', 't'): 7, ('t', '_'): 4, ('t', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('t', 'a'): 9, ('a', 'l'): 9, ('l', 'l'): 9, ('l', '_'): 5, ('l', 'e'): 4})\n",
      "merge #1: ('t', 'a')\n",
      "pairs ends: defaultdict(<class 'int'>, {('f', 'a'): 7, ('a', 's'): 7, ('s', 't'): 7, ('t', '_'): 4, ('t', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('ta', 'l'): 9, ('l', 'l'): 9, ('l', '_'): 5, ('l', 'e'): 4})\n",
      "merge #2: ('ta', 'l')\n",
      "pairs ends: defaultdict(<class 'int'>, {('f', 'a'): 7, ('a', 's'): 7, ('s', 't'): 7, ('t', '_'): 4, ('t', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('tal', 'l'): 9, ('l', '_'): 5, ('l', 'e'): 4})\n",
      "merge #3: ('tal', 'l')\n",
      "pairs ends: defaultdict(<class 'int'>, {('f', 'a'): 7, ('a', 's'): 7, ('s', 't'): 7, ('t', '_'): 4, ('t', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('tall', '_'): 5, ('tall', 'e'): 4})\n",
      "merge #4: ('f', 'a')\n",
      "pairs ends: defaultdict(<class 'int'>, {('fa', 's'): 7, ('s', 't'): 7, ('t', '_'): 4, ('t', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('tall', '_'): 5, ('tall', 'e'): 4})\n",
      "merge #5: ('fa', 's')\n",
      "pairs ends: defaultdict(<class 'int'>, {('fas', 't'): 7, ('t', '_'): 4, ('t', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('tall', '_'): 5, ('tall', 'e'): 4})\n",
      "merge #6: ('fas', 't')\n",
      "pairs ends: defaultdict(<class 'int'>, {('fast', '_'): 4, ('fast', 'e'): 3, ('e', 'r'): 7, ('r', '_'): 7, ('tall', '_'): 5, ('tall', 'e'): 4})\n",
      "merge #7: ('e', 'r')\n",
      "pairs ends: defaultdict(<class 'int'>, {('fast', '_'): 4, ('fast', 'er'): 3, ('er', '_'): 7, ('tall', '_'): 5, ('tall', 'er'): 4})\n",
      "merge #8: ('er', '_')\n",
      "pairs ends: defaultdict(<class 'int'>, {('fast', '_'): 4, ('fast', 'er_'): 3, ('tall', '_'): 5, ('tall', 'er_'): 4})\n",
      "merge #9: ('tall', '_')\n",
      "pairs ends: defaultdict(<class 'int'>, {('fast', '_'): 4, ('fast', 'er_'): 3, ('tall', 'er_'): 4})\n",
      "merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "# Process byte pair encoding iteratively\n",
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'merge #{i + 1}:', max_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2d50c",
   "metadata": {},
   "source": [
    "After 10 iterations of byte pair encoding, we can see that list symbols now contains 10 more symbols that are iteratively merged from other symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53c9211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1ed0093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fast_', 'fast er_', 'tall_', 'tall er_']\n"
     ]
    }
   ],
   "source": [
    "print(list(token_freqs.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:philonlp] *",
   "language": "python",
   "name": "conda-env-philonlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
